<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>ハイパーバイザの作り方～ちゃんと理解する仮想化技術～ 第２０回 bhyveにおける仮想ディスクの実装</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="header">
<h1 class="title"><p>ハイパーバイザの作り方～ちゃんと理解する仮想化技術～ 第２０回 bhyveにおける仮想ディスクの実装</p></h1>
</div>
<h1 id="はじめに">はじめに</h1>
<p>前回の記事では、bhyveにおける仮想NICの実装についてTAPデバイスを用いたホストドライバの実現方法を例に挙げ解説しました。</p>
<p>今回の記事では、bhyveにおける仮想ディスクの実装について解説していきます。</p>
<h1 id="bhyveにおける仮想ディスクの実装">bhyveにおける仮想ディスクの実装</h1>
<p>bhyveがゲストマシンに提供する仮想IOデバイスは、全てユーザプロセスである/usr/sbin/bhyve上に実装されています（図１）。</p>
<p>bhyveは実機上のディスクコントローラと異なり、ホストOSのファイルシステム上のディスクイメージファイルに対してディスクIOを行います。</p>
<p>これを実現するために、/usr/sbin/bhyve上の仮想ディスクコントローラは、ゲストOSからのIOリクエストをディスクイメージファイルへのファイルIOへ変換します。</p>
<p>以下に、ディスク読み込み手順と全体図（図１）を示します。</p>
<ol style="list-style-type: decimal">
<li>ゲストOSはvirtio-blkドライバを用いて、共有メモリ上のリングバッファにIOリクエストを書き込みます。そして、IOポートアクセスによってハイパーバイザにリクエスト送出を通知する。IOポートアクセスによってVMExitが発生し、CPUの制御がホストOSのvmm.koのコードに戻る。 bhyveの仮想ディスクコントローラのエミュレーションは、ユーザランドで行われています。vmm.koはこのVMExitを受けてioctlをreturnし/usr/sbin/bhyveへ制御を移す。</li>
<li>ioctlのreturnを受け取った/usr/sbin/bhyveは、仮想ディスクコントローラの共有メモリ上のリングバッファからリクエストを取り出します。</li>
<li>２で取り出したリクエストをパースし、ディスクイメージファイルにread()を行います。</li>
<li>読み出したデータを共有メモリ上のリングバッファに乗せ、ゲストOSに割り込みを送ります。</li>
<li>ゲストOSは割り込みを受け、リングバッファからデータを読み出します。</li>
</ol>
<div class="figure">
<img src="figures/part20_fig1.png" alt="ディスク読み込み手順" /><p class="caption">ディスク読み込み手順</p>
</div>
<p>書き込み処理では、リクエストと共にデータをリングバッファを用いて送りますが、それ以外は読み込みと同様です。</p>
<h1 id="virtio-blkのしくみ">virtio-blkのしくみ</h1>
<p>これまでに、準仮想化I/Oの仕組みとして、virtioとVirtqueue、virtio-netについて解説してきました。ここでは、ブロックデバイスを準仮想化する、virtio-blkについて解説を行います。</p>
<p>virtio-netは受信キュー、送信キュー、コントロールキューの３つのVirtqueueからなっていましたが、virtio-blkでは単一のVirtqueueを用います。これは、ディスクコントローラの挙動がNICとは異なり、必ずOSからコマンド送信を行った後にデバイスからレスポンスが返るという順序になるためです<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>。</p>
<p>ブロックIOのリクエストは連載第１２回の「ゲスト→ホスト方向のデータ転送方法」で解説した手順で送信されます。</p>
<p>virtio-blkでは１つのブロックIOリクエストに対して、以下のようにDescriptor<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>群を使用します。１個目のDescriptorはstruct virtio_blk_outhdr（表１）を指します。この構造体にはリクエストの種類、リクエスト優先度、アクセス先オフセットを指定します。２〜（ｎ−１）個目以降のDescriptorはリクエストに使用するバッファを指します。リクエストがreadな場合は読み込み結果を入れる空きバッファを、writeな場合は書き込むデータを含むバッファを指定します。</p>
<p>バッファのアドレスは物理アドレス指定になるため、仮想アドレスで連続した領域でも物理的配置がバラバラな状態な場合があります。これをサポートするためにバッファ用Descriptorを複数に別けて確保出来るようになっています。</p>
<p>struct virtio_blk_outhdrにはバッファ長のフィールドがありませんが、これはDescriptorのlenフィールドを用いてホストへ通知されます。ｎ個目のDescriptorは1byteのステータスコード（表２）のアドレスを指します。このフィールドはホスト側がリクエストの実行結果を返すために使われます。</p>
<table>
<caption>struct virtio_blk_outhdr</caption>
<thead>
<tr class="header">
<th align="left">type</th>
<th align="left">member</th>
<th align="left">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">u32</td>
<td align="left">type</td>
<td align="left">リクエストの種類（read=0x0, write=0x1, ident=0x8)</td>
</tr>
<tr class="even">
<td align="left">u32</td>
<td align="left">ioprio</td>
<td align="left">リクエスト優先度</td>
</tr>
<tr class="odd">
<td align="left">u64</td>
<td align="left">sector</td>
<td align="left">セクタ番号（オフセット値）</td>
</tr>
</tbody>
</table>
<table>
<caption>ステータスコード</caption>
<thead>
<tr class="header">
<th align="left">type</th>
<th align="left">member</th>
<th align="left">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">0</td>
<td align="left">OK</td>
<td align="left">正常終了</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">IOERR</td>
<td align="left">IOエラー</td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">UNSUPP</td>
<td align="left">サポートされないリクエスト</td>
</tr>
</tbody>
</table>
<h1 id="ディスクイメージへのio">ディスクイメージへのIO</h1>
<p>/usr/sbin/bhyveはvirtio-blkを通じてゲストOSからディスクIOリクエストを受け取り、ディスクイメージへ読み書きを行います。bhyveが対応するディスクイメージはRAW形式のみなので、ディスクイメージへの読み書きはとても単純です。ゲストOSから指定されたオフセット値とバッファ長をそのまま用いてディスクイメージへ読み書きを行えばよいだけです<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>。</p>
<p>それでは、このディスクイメージへのIOの部分についてbhyveのコードを実際に確認してみましょう。/usr/sbin/bhyveの仮想ディスクIO処理のコードをコードリスト１に示します。</p>
<h2 id="コードリストusrsbinbhyveの仮想ディスクio処理">コードリスト１，/usr/sbin/bhyveの仮想ディスクIO処理</h2>
<pre><code>/* ゲストOSからIO要求があった時に呼ばれる */
static void
pci_vtblk_proc(struct pci_vtblk_softc *sc, struct vqueue_info *vq)
{
    struct virtio_blk_hdr *vbh;
    uint8_t *status;
    int i, n;
    int err;
    int iolen;
    int writeop, type;
    off_t offset;
    struct iovec iov[VTBLK_MAXSEGS + 2];
    uint16_t flags[VTBLK_MAXSEGS + 2];
    /* iovに１リクエスト分のDescriptorを取り出し */
    n = vq_getchain(vq, iov, VTBLK_MAXSEGS + 2, flags);
                    〜 略 〜
        /* 一つ目のDescriptorはstruct virtio_blk_outhdr */
    vbh = iov[0].iov_base;
        /* 最後のDescriptorはステータスコード */
    status = iov[--n].iov_base;
                    〜 略 〜
    /* リクエストの種類 */
        type = vbh-&gt;vbh_type;
    writeop = (type == VBH_OP_WRITE);
    /* オフセットをsectorからbyteに変換 */
    offset = vbh-&gt;vbh_sector * DEV_BSIZE;
    /* バッファの合計長 */
    iolen = 0;
    for (i = 1; i &lt; n; i++) {
                    〜 略 〜
        iolen += iov[i].iov_len;
    }
                    〜 略 〜
    switch (type) {
    /* WRITEならpwritev()でiovの配列で表されるバッファリストからディスクイメージへ書き込み */
    case VBH_OP_WRITE:
        err = pwritev(sc-&gt;vbsc_fd, iov + 1, i - 1, offset);
        break;
    /* READならpreadv()でディスクイメージからiovの配列で表されるバッファリストへ読み込み */
    case VBH_OP_READ:
        err = preadv(sc-&gt;vbsc_fd, iov + 1, i - 1, offset);
        break;
    /* IDENTなら仮想ディスクのidentifyを返す */
    case VBH_OP_IDENT:
        /* Assume a single buffer */
        strlcpy(iov[1].iov_base, sc-&gt;vbsc_ident,
            min(iov[1].iov_len, sizeof(sc-&gt;vbsc_ident)));
        err = 0;
        break;
    default:
        err = -ENOSYS;
        break;
    }
                    〜 略 〜
    /* ステータスコードのアドレスにIOの結果を書き込む */
    if (err &lt; 0) {
        if (err == -ENOSYS)
            *status = VTBLK_S_UNSUPP;
        else
            *status = VTBLK_S_IOERR;
    } else
        *status = VTBLK_S_OK;

                    〜 略 〜
    /* ステータスコードを書き込んだ事を通知 */
    vq_relchain(vq, 1);
}</code></pre>
<h1 id="まとめ">まとめ</h1>
<p>今回は仮想マシンのストレージデバイスについて解説しました。 次回は、仮想マシンのコンソールデバイスについて解説します。</p>
<div class="references">

</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>NICではOSから何もリクエストを送らなくてもネットワーク上の他のノードからパケットが届くのでデータが送られてきます。このため、必ずOSからリクエストを送ってから届くというような処理にはなりません。<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Virtqueue上でデータ転送をおこなうための構造体。第１２回のVirtqueueの項目を参照。<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>QCOW2形式などのより複雑なフォーマットでは未使用領域を圧縮するため、ゲスト・ホスト間でオフセット値が一致しなくなり、またメタデータを持つ必要が出てくるのでRAWイメージと比較して複雑な実装になります。<a href="#fnref3">↩</a></p></li>
</ol>
</div>
</body>
</html>
